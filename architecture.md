# Zen Note - Local RAG Architecture & Technical Specification

## ğŸŒŸ Objective

Build a privacy-focused Retrieval-Augmented Generation (RAG) application that allows querying `.md` files using a **local LLM**, with a modern Next.js + TailwindCSS + shadcn UI. The system is private by default, runs entirely locally, and avoids using cloud APIs.

---

## ğŸ§± High-Level Architecture Overview

```
User â†”ï¸ Web UI (Next.js) â†”ï¸ Backend API (FastAPI) â†”ï¸ Local LLM + Vector Store â†”ï¸ Knowledge Sources
```

### Core Components

- **Frontend (UI)**: Built with Next.js, TailwindCSS, and shadcn. Provides a simple input box to ask questions and shows the result.
- **Backend (RAG API)**: Python FastAPI server that handles the question, performs document retrieval using a local vector store (FAISS), and invokes a local LLM.
- **LLM (Local)**: Runs via Ollama with Mistral 7B model. Local inference on user's machine.
- **Embedding Generator**: Uses `sentence-transformers` (`all-MiniLM-L6-v2`) to embed markdown chunks.
- **Vector Store**: FAISS for fast semantic search and similarity matching.
- **Document Processing**: Python script to load, chunk, and index knowledge sources.

---

## ğŸ” RAG Query Processing Pipeline

### Complete Query Flow

```
User Question â†’ Vector Search â†’ Context Retrieval â†’ LLM Prompt â†’ Response
```

**Detailed Step-by-Step Process**:

1. **ğŸ” Query Vectorization**
   ```
   User: "What is RAG?"
   â†“
   Embedding Model converts question to 384-dim vector
   ```

2. **ğŸ¯ Vector Similarity Search**
   ```
   Query Vector â†’ FAISS Index â†’ Top K Similar Chunks
   â†“
   Returns: Most relevant text chunks from knowledge base
   ```

3. **ğŸ“ Context Assembly**
   ```
   System assembles prompt:
   "Based on this context: [retrieved chunks]
   Answer the user's question: What is RAG?"
   ```

4. **ğŸ¤– LLM Generation**
   ```
   Ollama (Mistral) receives:
   - Original question
   - Relevant context from documents
   â†“
   Generates answer grounded in user's knowledge base
   ```

**Critical Design Principle**: 
- ğŸš« **LLM doesn't query the vector DB** - vector search happens BEFORE the LLM
- âœ… **Vector search provides context, LLM provides generation**
- ğŸ¯ **Responses are grounded in user's documents, not general training data**

---

## ğŸ—‚ Project Structure

```
zen_note/
â”‚
â”œâ”€â”€ backend/                   # FastAPI backend
â”‚   â”œâ”€â”€ main.py                # FastAPI app with /ask endpoint
â”‚   â”œâ”€â”€ rag.py                 # RAG logic (embedding search + LLM call)
â”‚   â”œâ”€â”€ model_runner.py        # Wrapper to call the local LLM
â”‚   â”œâ”€â”€ embeddings.py          # Markdown parsing + embeddings
â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”œâ”€â”€ vector_store.faiss     # FAISS index file
â”‚   â””â”€â”€ vector_store_metadata.pkl # Chunk metadata and source information
â”‚
â”œâ”€â”€ frontend/                  # Next.js UI
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ index.tsx          # Main UI with question input + answer display
â”‚   â”œâ”€â”€ components/            # shadcn components
â”‚   â”œâ”€â”€ app/                   # App routing (Next.js 13+)
â”‚   â””â”€â”€ tailwind.config.js     # Tailwind setup
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ markdowns/             # Folder for all your `.md` knowledge files
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_index.py         # Script to parse sources, generate embeddings, store FAISS
â”‚   â””â”€â”€ watch_and_index.py     # (Future) Dynamic index updates
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sources.yaml           # (Future) Multi-source configuration
â”‚
â”œâ”€â”€ tasks.md                   # Detailed task plan for MVP
â”œâ”€â”€ .env                       # Local config (model path, ports, etc.)
â”œâ”€â”€ requirements.txt           # Backend dependencies
â”œâ”€â”€ README.md                  # User documentation and setup
â””â”€â”€ architecture.md            # (This file) Technical architecture reference
```

---

## âš™ï¸ Technology Stack & Technical Decisions

### Current Implementation (v0)

|Component|Technology|Rationale|
|---|---|---|
|**LLM**|Ollama + Mistral 7B|Better macOS integration, optimal performance/resource ratio|
|**Embedding Model**|`all-MiniLM-L6-v2`|Balanced performance and quality, 384-dimension vectors|
|**Vector DB**|FAISS IndexFlatIP|Exact cosine similarity search, high performance|
|**Backend API**|FastAPI|Fast, modern Python API framework|
|**Frontend**|Next.js + TailwindCSS + shadcn|Modern React framework with excellent UI components|
|**Document Processing**|Custom chunking + sentence-transformers|1000 char chunks with 200 char overlap for optimal retrieval|

### Configuration Management

**Centralized in `backend/config.py`**:
- **OLLAMA_HOST**: `http://localhost:11434`
- **OLLAMA_MODEL**: `mistral`
- **EMBEDDINGS_MODEL**: `all-MiniLM-L6-v2`
- **CHUNK_SIZE**: 1000 characters
- **CHUNK_OVERLAP**: 200 characters

---

## ğŸ“Š Current System Status

### âœ… Completed Tasks

**Task 1 - Local LLM Setup**:
- Ollama installation and configuration
- Mistral 7B model download and testing
- HTTP API integration verified
- Service management scripts

**Task 2 - FAISS Index Setup**:
- Sample markdown knowledge base created
- Comprehensive indexing script with chunking
- 384-dimension embeddings with high-quality semantic search
- Index files: `vector_store.faiss` + `vector_store_metadata.pkl`

### Current Index Performance
- **8 chunks** from 2 sample files
- **384-dimension** embeddings
- **High-quality semantic search** with relevance scoring

---

## ğŸš€ Future Architecture (v1+ Requirements)

### Dynamic Index Management ğŸ”„

**Current Limitation**: Manual rebuild required for content changes

**Proposed Dynamic System**:
- **File Watcher Service**: Automatically detect markdown file changes
- **Incremental Updates**: Only re-index changed files instead of full rebuild
- **Smart Chunking**: Detect which specific chunks changed within files
- **Background Processing**: Update index without service interruption
- **Version Control**: Track index versions with rollback capability

**Future Implementation**:
```bash
# File watcher daemon
python3 scripts/watch_and_index.py --daemon

# Incremental updates
python3 scripts/build_index.py --incremental --changed-files file1.md,file2.md

# Scheduled rebuilds
python3 scripts/build_index.py --schedule hourly
```

### Multi-Source Knowledge Integration ğŸ“š

**Current State**: Local markdown files only

**v1 Multi-Source Architecture**:
```
Data Sources â†’ Source Adapters â†’ Common Format â†’ Chunking â†’ Vector Store
     â†“              â†“              â†“           â†“         â†“
  Notion        NotionAdapter    Markdown    Chunker   FAISS
  GitHub        GitHubAdapter    Chunks      Embedder  Index
  PDFs          PDFAdapter       Metadata    Storage   Search
  Confluence    ConfluenceAdapter
  Google Docs   GoogleDocsAdapter
```

**Supported Sources (Future)**:
- **Notion**: Workspace pages and databases
- **GitHub**: Repository markdown files
- **Confluence**: Corporate wiki content
- **Google Docs**: Shared documents
- **PDFs**: Document processing and OCR
- **Web Scraping**: Website content extraction
- **Email Archives**: Conversation indexing

**Source Adapter Interface**:
```python
# Abstract base class for all source adapters
class SourceAdapter:
    def fetch_documents(self) -> List[Document]
    def detect_changes(self) -> List[str]
    def get_metadata(self, doc_id: str) -> Dict
```

**Configuration System (Future)**:
```yaml
# config/sources.yaml
sources:
  - type: "notion"
    workspace_id: "your-workspace"
    auth_token: "env:NOTION_TOKEN"
  - type: "github"
    repo: "user/knowledge-repo"
    path: "docs/"
  - type: "local"
    path: "data/markdowns/"
```

---

## ğŸ—ï¸ Development Phases

### Phase 0 (Current) - MVP Core
- âœ… Local LLM (Ollama + Mistral)
- âœ… FAISS vector store
- â³ FastAPI backend
- â³ Next.js frontend
- â³ Basic RAG pipeline

### Phase 1 - Enhanced Functionality
- ğŸ”„ Dynamic index updates
- ğŸ§  Conversation history
- ğŸ” Source attribution in responses
- âš™ï¸ Model configuration UI

### Phase 2 - Multi-Source Integration
- ğŸ“š Notion connector
- ğŸ“ GitHub repository sync
- ğŸ“„ PDF processing
- ğŸŒ Web content scraping

### Phase 3 - Advanced Features
- ğŸ” User authentication
- ğŸ‘¥ Multi-user support
- ğŸ“Š Analytics and usage tracking
- ğŸ›ï¸ Advanced configuration management

---

## âœ… v0 Features (MVP)

- âœ… **Privacy-First**: 100% local processing, no cloud APIs
- âœ… **Fast Semantic Search**: FAISS-powered vector similarity
- âœ… **Local LLM**: Ollama-based inference
- â³ **Clean UI**: Next.js + TailwindCSS + shadcn
- â³ **Simple RAG**: Question â†’ Context â†’ Answer pipeline

## ğŸ¯ v1 Preview Features

- ğŸ”„ **Dynamic Indexing**: Auto-update on content changes
- ğŸ“š **Multi-Source**: Notion, GitHub, PDF support
- ğŸ§  **Conversation Memory**: Follow-up question chaining
- ğŸ” **Source Attribution**: Show markdown snippets in responses
- âš™ï¸ **Model Flexibility**: Swap LLMs via UI
- ğŸ›ï¸ **Advanced Config**: Source management interface

---

## ğŸ”’ Privacy & Security Architecture

### Data Privacy
- **100% Local Processing**: No data leaves user's machine
- **No External APIs**: All models and processing local
- **Private by Design**: Documents and queries stay private
- **No Telemetry**: No usage tracking or data collection

### Security Considerations
- **Local Network Only**: Backend API not exposed externally
- **File System Access**: Controlled access to knowledge directories
- **Model Security**: Local models, no remote inference calls

---

## ğŸ Next Development Tasks

### Immediate (Task 3-5)
1. **FastAPI Backend**: Implement RAG pipeline API
2. **Next.js Frontend**: Create minimal query interface
3. **Integration**: Connect frontend â†” backend â†” LLM

### Future Development
1. **Dynamic Indexing**: File watcher implementation
2. **Multi-Source**: Notion API integration
3. **UI Enhancement**: Source attribution, conversation history
4. **Performance**: Optimization for larger knowledge bases

---

## ğŸ“š Reference Resources

- [Ollama Documentation](https://ollama.ai/docs)
- [FAISS Documentation](https://faiss.ai/)
- [Sentence Transformers](https://www.sbert.net/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Next.js Documentation](https://nextjs.org/docs)
- [Mistral AI](https://mistral.ai/)

---

**Status**: Local LLM âœ… | FAISS Index âœ… | FastAPI Backend â³ | Next.js UI â³ | Integration â³